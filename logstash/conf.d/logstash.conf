#
input {
  tcp {
    port => 514
    type => syslog
  }
  tcp {
    port => 5000
    codec => json
    tags => "syslog"
  }
  tcp {
    port => 6000
    tags => "jog4j"
  }
 beats {
    port => 7000
    tags => "elb"
  }

  # fluentd codec support is currently broken
  # https://github.com/logstash-plugins/logstash-codec-fluent/issues/2
  # https://github.com/logstash-plugins/logstash-codec-fluent/pull/5
  # tcp {
  #  port => 24224
  #  codec => fluent
  # }
  gelf {
    port => 12201
    type => gelf
  }
}

# for all message types we're going to first parse out the body and then
# grok on that to get the actual Nginx log format
filter {

  if [type] == "syslog" {
    grok {
      match => {
        "message" => '%{SYSLOG5424PRI:syslog5424_pri}+(?:%{TIMESTAMP_ISO8601:syslog_timestamp}|-) %{WORD} %{WORD}/+(?:%{HOSTNAME:containerid}|-)\[+(?:%{POSINT:pid}|-)\]: %{GREEDYDATA:msg}'
      }
    }
    syslog_pri { }
    date { match => [ "syslog_timestamp", "ISO8601"] }
    if !("_grokparsefailure" in [tags]) {
      mutate {
        remove_field => [ "@source_host", "@message" ]
      }
    }
    mutate {
      remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
    }
  } else if [type] == "gelf" {
    grok {
      match => {
        "short_message" => '%{GREEDYDATA:msg}'
      }
    }
  }
  grok {
    #Pull out the JSON from the node logger message passed in through syslog
    match => {"message"=>"[ :/\[\]<>a-zA-Z0-9._-]+(?<loggerContents>\{.*)"}
    add_tag => "loggerJsonFound"
  }
  if "loggerJsonFound" in [tags] {
    json {
      source => "loggerContents"
    }
#    mutate {
#      remove_tag => [loggerJsonFound]
#    }
  }
  if "_grokparsefailure" in [tags] {
    mutate {
      remove_tag => [ "_grokparsefailure" ]
      add_tag => "noneLoggerMessage"
    }
  }

  # match ELB
  if "elb" in [tabs] {
    grok {
      match => ["message", "%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} %{ELB_REQUEST_LINE}"]
    }
  }

  # match nginx_access first
  grok {
    match => {
    "msg" => '%{IP:client} - (?:%{NOTSPACE:http_user}|-) \[%{HTTPDATE:http_timestamp}\] "%{WORD:http_method} %{URIPATHPARAM:http_request} HTTP/%{NUMBER:http_version}" %{NUMBER:http_code} %{NUMBER:http_bytes_sent} (?:%{QUOTEDSTRING:http_referer}|-) (?:%{QUOTEDSTRING:http_user_agent}|-) "(?:%{IP:x_forwarded_for}|-)"+%{GREEDYDATA:msg}'
    }
    add_tag => "nginx_access"
  }
  # failed to match nginx_access log, so parse as error
  if "_grokparsefailure" in [tags] {
    mutate {
      remove_tag => [ "_grokparsefailure" ]
      remove_field => [ "level", "timestamp" ]      
      add_tag => "error"
    }
  } else {
      mutate {
        # all this is redundant data at this point
        remove_field => [ "message", "msg", "short_message" ]
      }
  }


}

output {
  elasticsearch { hosts => ["elasticsearch"] }
  stdout { codec => rubydebug }
}
